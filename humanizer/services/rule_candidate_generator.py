"""
Rule-Based Candidate Generator - Week 7 Hybrid Architecture

Generates transformation candidates using learned patterns from Week 6.
Strategy: Apply high-reliability patterns to create diverse candidates quickly.
"""

import json
import re
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)


@dataclass
class RuleCandidate:
    """A single candidate generated by rule application."""
    text: str
    rule_description: str
    patterns_applied: List[str]  # e.g., ["remove:I", "remove:think"]
    confidence: float  # Based on rule reliability (high=1.0, medium=0.5)


class RuleCandidateGenerator:
    """
    Generates transformation candidates using learned patterns.

    Strategy:
    - Apply high-reliability patterns first (count >= 2)
    - Combine multiple patterns for diversity
    - Ensure candidates differ by >10% word overlap
    - Fast: No LLM calls, pure pattern matching (~0.1s for 8 candidates)
    """

    def __init__(
        self,
        rules_path: Path = None,
        min_reliability: str = "medium"  # "high" or "medium"
    ):
        """
        Initialize with learned rules.

        Args:
            rules_path: Path to extracted_rules.json
            min_reliability: Minimum rule reliability ("high" requires count >= 2)
        """
        if rules_path is None:
            rules_path = Path("data/transformation_rules/extracted_rules.json")

        self.rules = self._load_rules(rules_path)
        self.min_reliability = min_reliability

        logger.info(f"Loaded rules for {len(self.rules)} pack/axis combinations")

    def generate_candidates(
        self,
        text: str,
        pack_name: str,
        target_axis: str,
        num_candidates: int = 8
    ) -> List[RuleCandidate]:
        """
        Generate rule-based transformation candidates.

        Args:
            text: Original text to transform
            pack_name: POVM pack name (e.g., "tetralemma")
            target_axis: Target axis (e.g., "A", "¬A")
            num_candidates: Target number of candidates (default 8)

        Returns:
            List of RuleCandidate objects, sorted by confidence

        Strategy:
        1. Get rules for this pack/axis
        2. Apply high-reliability substitutions
        3. Apply high-reliability removals (single + combinations)
        4. Apply high-reliability additions (especially negations)
        5. Apply medium-reliability patterns if needed
        6. Ensure diversity (>10% word overlap difference)
        7. Return top N candidates by confidence
        """
        axis_key = f"{pack_name}/{target_axis}"

        if axis_key not in self.rules:
            logger.warning(f"No rules found for {axis_key}")
            return []

        axis_rules = self.rules[axis_key]
        candidates = []

        # 1. Apply high-reliability substitutions
        for sub in axis_rules["substitutions"]:
            if sub["reliability"] == "high" or self.min_reliability == "medium":
                candidate_text = self._apply_substitution(
                    text, sub["from"], sub["to"]
                )
                if candidate_text != text:
                    confidence = 1.0 if sub["reliability"] == "high" else 0.5
                    candidates.append(RuleCandidate(
                        text=candidate_text,
                        rule_description=f"substitute: '{sub['from']}' → '{sub['to']}'",
                        patterns_applied=[f"sub:{sub['from']}→{sub['to']}"],
                        confidence=confidence
                    ))

        # 2. Apply high-reliability removals (single words)
        high_rel_removals = [
            r for r in axis_rules["removals"]
            if r["reliability"] == "high" or self.min_reliability == "medium"
        ]

        for removal in high_rel_removals:
            candidate_text = self._apply_removal(text, removal["word"])
            if candidate_text != text:
                confidence = 1.0 if removal["reliability"] == "high" else 0.5
                candidates.append(RuleCandidate(
                    text=candidate_text,
                    rule_description=f"remove: '{removal['word']}'",
                    patterns_applied=[f"remove:{removal['word']}"],
                    confidence=confidence
                ))

        # 3. Apply combinations of high-reliability removals
        if len(high_rel_removals) >= 2:
            # Try removing 2 high-reliability words together
            for i, rem1 in enumerate(high_rel_removals[:3]):  # Limit to top 3
                for rem2 in high_rel_removals[i+1:4]:  # Limit combinations
                    candidate_text = self._apply_removal(text, rem1["word"])
                    candidate_text = self._apply_removal(candidate_text, rem2["word"])

                    if candidate_text != text:
                        confidence = 1.0 if rem1["reliability"] == "high" and rem2["reliability"] == "high" else 0.75
                        candidates.append(RuleCandidate(
                            text=candidate_text,
                            rule_description=f"remove: '{rem1['word']}', '{rem2['word']}'",
                            patterns_applied=[f"remove:{rem1['word']}", f"remove:{rem2['word']}"],
                            confidence=confidence
                        ))

        # 4. Apply high-reliability additions (especially negations)
        for addition in axis_rules["additions"]:
            if addition["reliability"] == "high" or self.min_reliability == "medium":
                # Special handling for negations
                if addition["word"] in ["not", "no"]:
                    candidate_text = self._insert_negation(text, addition["word"])
                else:
                    # For other additions, skip (too context-dependent)
                    continue

                if candidate_text != text:
                    confidence = 1.0 if addition["reliability"] == "high" else 0.5
                    candidates.append(RuleCandidate(
                        text=candidate_text,
                        rule_description=f"add negation: '{addition['word']}'",
                        patterns_applied=[f"add:{addition['word']}"],
                        confidence=confidence
                    ))

        # 5. Apply substitution + removal combinations (for more diversity)
        # Try top substitution + top removal
        if axis_rules["substitutions"] and high_rel_removals:
            top_sub = axis_rules["substitutions"][0]
            top_rem = high_rel_removals[0]

            candidate_text = self._apply_substitution(text, top_sub["from"], top_sub["to"])
            candidate_text = self._apply_removal(candidate_text, top_rem["word"])

            if candidate_text != text:
                candidates.append(RuleCandidate(
                    text=candidate_text,
                    rule_description=f"substitute '{top_sub['from']}' + remove '{top_rem['word']}'",
                    patterns_applied=[f"sub:{top_sub['from']}", f"remove:{top_rem['word']}"],
                    confidence=0.75
                ))

        # 6. Ensure diversity and return top N
        unique_candidates = self._ensure_diversity(candidates, min_overlap_diff=0.10)

        # Sort by confidence (highest first)
        unique_candidates.sort(key=lambda c: c.confidence, reverse=True)

        # Return top N candidates
        result = unique_candidates[:num_candidates]

        logger.info(f"Generated {len(result)} rule-based candidates for {axis_key}")

        return result

    def _apply_substitution(self, text: str, from_phrase: str, to_phrase: str) -> str:
        """
        Apply word/phrase substitution (case-insensitive, word boundaries).

        Args:
            text: Original text
            from_phrase: Phrase to replace
            to_phrase: Replacement phrase

        Returns:
            Transformed text (or original if pattern not found)
        """
        # Use word boundaries for single words, case-insensitive
        if ' ' not in from_phrase:
            pattern = r'\b' + re.escape(from_phrase) + r'\b'
            return re.sub(pattern, to_phrase, text, flags=re.IGNORECASE, count=1)
        else:
            # Multi-word phrase: case-insensitive replace
            return re.sub(
                re.escape(from_phrase),
                to_phrase,
                text,
                flags=re.IGNORECASE,
                count=1
            )

    def _apply_removal(self, text: str, word: str) -> str:
        """
        Remove word from text (case-insensitive, preserve spacing).

        Args:
            text: Original text
            word: Word to remove

        Returns:
            Text with word removed and spacing cleaned up
        """
        # Remove word with word boundaries
        pattern = r'\b' + re.escape(word) + r'\b\s*'
        result = re.sub(pattern, '', text, flags=re.IGNORECASE, count=1)

        # Clean up multiple spaces and strip
        result = re.sub(r'\s+', ' ', result).strip()

        return result

    def _insert_negation(self, text: str, negation_word: str) -> str:
        """
        Insert negation word intelligently.

        Strategy: Insert after first modal verb (should, could, will, etc.)
        Falls back to no change if no modal found.

        Args:
            text: Original text
            negation_word: Negation to insert ("not", "no")

        Returns:
            Text with negation inserted (or original if no good insertion point)
        """
        # Common modals for negation insertion
        modals = ['should', 'could', 'would', 'will', 'can', 'may', 'might', 'must']

        for modal in modals:
            pattern = r'\b(' + re.escape(modal) + r')\b'
            match = re.search(pattern, text, flags=re.IGNORECASE)
            if match:
                # Insert negation after modal
                result = text[:match.end()] + ' ' + negation_word + text[match.end():]
                return result

        # No modal found - return original
        return text

    def _ensure_diversity(
        self,
        candidates: List[RuleCandidate],
        min_overlap_diff: float = 0.10
    ) -> List[RuleCandidate]:
        """
        Filter candidates to ensure diversity.

        Args:
            candidates: List of rule candidates
            min_overlap_diff: Minimum difference in word overlap (0.10 = 10%)

        Returns:
            Filtered list with diverse candidates only

        Strategy:
        - Keep first occurrence (highest confidence sorted first)
        - Remove candidates with >90% word overlap with existing candidates
        """
        unique_candidates = []

        for candidate in candidates:
            is_too_similar = False

            for existing in unique_candidates:
                overlap = self._calculate_word_overlap(candidate.text, existing.text)
                if overlap > (1.0 - min_overlap_diff):  # >90% overlap
                    is_too_similar = True
                    break

            if not is_too_similar:
                unique_candidates.append(candidate)

        logger.debug(f"Diversity filter: {len(candidates)} → {len(unique_candidates)} candidates")

        return unique_candidates

    def _calculate_word_overlap(self, text1: str, text2: str) -> float:
        """
        Calculate word overlap ratio (0-1).

        Formula: overlap = |words1 ∩ words2| / max(|words1|, |words2|)

        Args:
            text1: First text
            text2: Second text

        Returns:
            Overlap ratio (0.0 = no overlap, 1.0 = identical)
        """
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())

        if not words1 or not words2:
            return 0.0

        intersection = len(words1 & words2)
        max_length = max(len(words1), len(words2))

        return intersection / max_length

    def _load_rules(self, rules_path: Path) -> Dict:
        """
        Load rules from JSON file.

        Args:
            rules_path: Path to extracted_rules.json

        Returns:
            Dictionary of rules by pack/axis
        """
        with open(rules_path, 'r') as f:
            rules = json.load(f)

        return rules

    def get_supported_axes(self) -> List[str]:
        """
        Get list of pack/axis combinations with learned rules.

        Returns:
            List of axis keys (e.g., ["tetralemma/A", "tetralemma/¬A"])
        """
        return list(self.rules.keys())
